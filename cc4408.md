
Informal proposal:

A layer of abstraction built on top of puppeteer and puppeteer-cluster with an emphasis on the whole pipeline from data extraction to processing, storage handling, archival.


My idea is basically to keep working on the project that I was working on during my free time. It can be viewed as an opinionated web scraping framework in nodejs on top of puppeteer which is a nodejs library used to control the chromium browser. The framework, basically automates a bunch of mundane tasks that are reapeted in almost every web scraping project and it makes it easier for me to extract, process, debug and finally archieve data from the web.

Why did I create it?
Out of frustration for certain repeated tasks, and also some pain points regarding the debuggability of all sorts of weird obstucles on different websites.


What is there to be implemented yet:
While I have a lot of things in mind, the main things that I was thinking of adding for now are:
1) External downloader (external to chrome) that seamlessly integrates with chrome in headless mode.
2) Logic for zipping and categorizing past data.

Challenges:

I am not sure if this is the correct project for this course in the sense that it might prove to be difficult to effectively provide 100% coverage for a library that is heavily dependent on a headless browser? We will see.




