
A layer of abstraction built on top of puppeteer cluster with an emphasis on data extraction, processing, storage handling. 


My idea is basically to keep working on the project that I was working on during my free time. It can be viewed as an opinionated web scraping framework in nodejs on top of puppeteer which is a nodejs library used to control the chromium browser. The framework, basically automates a bunch of mundane tasks that are reapeted in almost every web scraping project and it makes it easier for me to extract, process, debug and finally archieve web data.

why did I create it?
Out of frustration for certain repeated tasks, and also some pain points regarding the debuggability of all sorts of weird ob\stucles on different websites.


What is there to be implemented yet:
]WhilemI have a lot of thinks in mind, the main things that I was thinking of adding for now are:
1) Independed downloader (From chrome) that seamlessly integrates with chrome in headless mode.
2) Logic for zipping and categorizing past data

Challenges:

I am not sure if this is the correct project for this course in the sense that it might prove to be difficult to effectively provide 100% coverage for a library that is heavily dependent on a headless browser? We will see.




